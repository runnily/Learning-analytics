## For multiple linear regression, different criteria often suggest different models are best
## and this is the case here.
## In order to see the difference between the best model we can generate a plot
par(mfrow=c(1,2))
## produce plots, highlighting optimal value of k
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab = "Number of predictors", ylab = "BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
## Get the best subset
bss_fit_AIC$Subsets[6,]
cancer_data_bss = cancer_data[,c("Cl.thickness", "Marg.adhesion", "Bare.nuclei", "Bl.cromatin", "Normal.nucleoli", "y")]
lr_1 = glm(formula = y ~ ., family = "binomial", data = cancer_data_bss)
summary(lr_1)
## -------------- LASSO selection ------------------
par(mfrow=c(1,1))
## Here we want to using the LASSO to find the best optimal subset.
## Choose grid of values for the tuning parameter
grid = 10^seq(4,-4, length=100)
## Fit a model with LASSO penalty for each value of the tuning parameter
lasso_fit = glmnet(X, y, family="binomial", alpha=1, standardize=FALSE, lambda=grid)
plot(lasso_fit, xvar="lambda", col=rainbow(p), label=TRUE)
lasso_cv_fit = cv.glmnet(X, y, family="binomial", alpha=1, standardize=FALSE, lambda=grid, type.measure="class")
plot(lasso_cv_fit)
which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min)
lambda_lasso_min = lasso_cv_fit$lambda.min
coef(lasso_fit, s=lambda_lasso_min)
## -------------- ridge regression selection ------------------
## Fit a model with LASSO penalty for each value of the tuning parameter
ridge_fit = glmnet(X, y, family="binomial", alpha=0, standardize=FALSE, lambda=grid)
plot(ridge_fit, xvar="lambda", col=rainbow(p), label=TRUE)
ridge_cv_fit = cv.glmnet(X, y, family="binomial", alpha=0, standardize=FALSE, lambda=grid, type.measure="class")
plot(ridge_cv_fit)
ridge_min = ridge_cv_fit$lambda.min
coef(ridge_fit, s=ridge_min)
## ------------- Calculates the test error -----------------
predictedProb = function(phat, y, yhat=NULL) {
#' Provides the testing error from a given
#' set of prediction, including a an output
#' of a concussion matrix.
#'
#'
#' @description This is done by taking the predictions
#'
#' @param phat The prediction
#' @param y The actual data
#' @param yhat The prediction
#'
#' @return The parameter theta
if (is.null(yhat) == TRUE) {
yhat = ifelse(phat > 0.5, 1, 0)}
print(table(Observed=y, Predicted=yhat))
return(1-mean(y==yhat))
}
## ------------- Linear discriminant analysis -----------------
lda_fit = linDA(variables = within(cancer_data, rm(y)), group = cancer_data$y)
lda_fit$functions
x = seq(-1, 0, 0.001)
x1 = seq(0, 1, 0.001)
Q1 = -1.8751-0.8757*x -0.6555*x -0.4576*x-0.2312*x -0.2193*x-1.6190*x-0.4599*x-0.5537*x-0.0166*x
Q2 = -6.0351 + 1.6269*x1+1.2177*x1+0.8500*x1+0.4294*x1+0.4073*x1+3.0077*x1+0.8543*x1+1.0287*x1+0.0308*x1
plot(x, Q2, type="l", col="red", xlab="x", ylab="Discriminant function")
lines(x, Q1, col="blue")
lda_train = lda(y ~ ., cancer_data)
## ------------- Quadratic discriminant analysis -----------------
qda_fit = quaDA(variables = within(cancer_data, rm(y)), group = cancer_data$y, functions = TRUE)
qda_train = qda(y ~ ., cancer_data)
qda_fit$functions
x_1 = seq(-5, 5, 0.001)
x_2 = seq(-5, 5, 0.001)
x_3 = seq(-5, 5, 0.001)
x_4 = seq(-5, 5, 0.001)
x_5 = seq(-5, 5, 0.001)
x_6 = seq(-5, 5, 0.001)
x_7 = seq(-5, 5, 0.001)
x_8 = seq(-5, 5, 0.001)
x_9 = seq(-5, 5, 0.001)
Q_1 = -0.719382*(x_1 )^2-1.7058039*(x_2 )^2-3.4937973*(x_3 )^2-0.58232003*(x_4 )^2-1.065855928*(x_5 )^2-0.8966413*(x_6 )^2-1.744926559*(x_7 )^2-0.84732991*(x_8 )^2-0.755990642*(x_9 )^2+ 0.5162133*(x_2 )+ 0.3404121*(x_3 )+ 0.2636177*(x_4 )+ 0.3156083*(x_5 )+ 1.5235343*(x_6 )+0.3832991*(x_7 )+ 0.5544928*(x_8 )-0.1372386*(x_9)-2.88806
Q_2 = -1.653368*(x_1 )^2-16.1466469*(x_2 )^2-24.779313*(x_3 )^2-7.67591883*(x_4 )^2-8.39889256*(x_5 )^2-16.5954618*(x_6)^2-4.52243149*(x_7)^2-20.43116132*(x_8 )^2-6.96453663*(x_9 )^2-0.1834093*(x_1 )  -2.8394269 *(x_2)-1.9059405*(x_3 )  -2.3386108*(x_4) -0.1733111*(x_5) -1.9083752*(x_6 )-1.4636685*(x_7) -1.1395618*(x_8 )  -3.0913668*(x_9 )+ 6.052617
plot(x_1, Q_1, type="l", col="red", xlab="x", ylab="Discriminant function")
lines(x_1, Q_2, col="blue")
## ------------- Cross validation -----------------
cross_valid = function(df = cancer_data, m = 1) {
#' Provides the testing error using cross validation
#'
#' @description There are 6 different models to select
#' from (1-6) specified using m. The model selected is
#' where cross validation will be applied:
#' The idea of the cross validation is as follows:
#' 1. Take the group as a holdout or test data set
#' 2. Take the remaining groups as a training data set
#' 3. Fit a model on the training set and evaluate it on the test set
#' 4. Retain the evaluation score and discard the model
#'
#' @param df The dataframe where cross validation is to be applied.
#' @param m The model selection include
#'        (1) Normal logistic regression
#'        (2) Logistic regression with best subset selection
#'        (3) Logistic regression with LASSO
#'        (4) Logistic regression with ridge regression
#'        (5) Discriminant based approach using LDA
#'        (6) Discriminant based approach using QDA
#'
#' @return list of evaluations for each n fold
evaluations = rep(0, 10)
# This is a list of splits to perform our training data and test data
split_num = list(c(1,69), c(69,137), c(137, 205), c(205,273), c(273,341), c(341,409), c(409,477), c(477,545),c(545,613),c(613, 683))
for ( i in seq(1:10) ) {
split = split_num[[i]]
test_data = df[split[1]:split[2],]
train_data = df[-(split[1]:split[2]),]
# The following if statement will check the mode
if (m == 1) {
model = glm(formula = y ~ ., family = "binomial", data = train_data)
evaluations[i] = predictedProb(predict(model, test_data, type="response"), test_data$y)
}
if (m == 2) {
model = glm(formula = y ~ ., family = "binomial", data = train_data[,c("Cl.thickness", "Marg.adhesion", "Bare.nuclei", "Bl.cromatin", "Normal.nucleoli", "y")] )
evaluations[i] = predictedProb(predict(model, test_data[,c("Cl.thickness", "Marg.adhesion", "Bare.nuclei", "Bl.cromatin", "Normal.nucleoli", "y")], type="response"), test_data$y)
}
if (m == 3) {
grid = 10^seq(4,-4, length=100)
data = within(train_data, rm(y))
model = glmnet(as.matrix(data), as.matrix(train_data$y), family="binomial", alpha=1, standardize=FALSE, lambda=grid)
fit = cv.glmnet(as.matrix(data), as.matrix(train_data$y), family="binomial", alpha=1, standardize=FALSE, lambda=grid, type.measure="class")
lambda_min = fit$lambda.min
evaluations[i] = predictedProb(predict(model, as.matrix(within(test_data, rm(y))), s=lambda_min, type="response"), test_data$y)
}
if (m == 4) {
grid = 10^seq(4,-4, length=100)
data = within(train_data, rm(y))
model = glmnet(as.matrix(data), as.matrix(train_data$y), alpha=0, family="binomial", standardize=FALSE, lambda=grid)
fit = cv.glmnet(as.matrix(data), as.matrix(train_data$y), alpha=0, family="binomial", standardize=FALSE, lambda=grid, type.measure="class")
lambda_min = fit$lambda.min
evaluations[i] = predictedProb(predict(model, as.matrix(within(test_data, rm(y))), s=lambda_min, type="response"), test_data$y)
}
if (m == 5) {
model = lda(y ~ ., train_data)
testing = predict(model, test_data, type="response")
evaluations[i] = predictedProb(testing, test_data$y, testing$class )
}
if (m == 6) {
model = qda(y ~ ., train_data)
testing = predict(model, test_data, type="response")
evaluations[i] = predictedProb(testing, test_data$y, testing$class )
}
}
return(evaluations)
}
## ---------------- Predicted using full set ---------------------
predictedProb(predict(lasso_fit, X, s=lambda_lasso_min, type="response"), y)
predictedProb(predict(ridge_fit, X, s=ridge_min, type="response"), y)
predictedProb(predict(lr_1, cancer_data_bss, type="response"), y)
lda_testing = predict(lda_train, cancer_data, type="response")
predictedProb(lda_testing, y, lda_testing$class )
qda_testing = predict(qda_train, cancer_data, type="response")
predictedProb(qda_testing, y, qda_testing$class )
## --------------- Predicted using cv ----------------------
num = cross_valid(m=2)
num
sum(num)/10
nrow(cancer_data[-(613:683),])
length(cancer_data$y[cancer_data$y == 1])
qda(y ~ ., cancer_data)
lda(y ~ ., cancer_data)
lda(y ~ ., cancer_data)
qda(y ~ ., cancer_data)
View(cancer_data)
plot(lasso_fit, xvar="lambda", col=rainbow(p), label=TRUE)
load("~/Documents/Msc Data science and AI/MAS8404 - Statistical Learning for Data Science /Assignment/env.RData")
## Read in the data
## the Ch10Ex11 file is imported already using (file > import dataset)
set.seed(1)
gexpr = t(Ch10Ex11)
## loading package used in part 2: linear regression
library(glmnet)
library(ncl)
## Cluster Analysis
## 1A
## find the correlation distance
## Transpose the data into a matrix, as correlation is between the
## observation (p) and not variables (individuals)
gexpr_t = t(gexpr)
## Compute the observation correlations:
correlation_obs = cor(gexpr_t)
## To get distance matrix subtract by 1
dist_correlation = as.dist(1 - correlation_obs)
## Applying single-linkage using the correlation based correlation
## which was calculated
hc_s = hclust(dist_correlation, method="single")
plot(hc_s, cex=0.5, main="", sub="", xlab="")
## This visualizes the cluster solution using principle components
pca_gexpr = prcomp(x=gexpr, scale = TRUE)
hc_s_cut = cutree(hc_s, h=0.9553)
plot(pca_gexpr$x[,1], pca_gexpr$x[,1], xlab="First PC", ylab="Second PC", col=hc_s_cut, pch=hc_s_cut)
## 1B
## Applying hierarchical clustering using complete linkage
hc_c = hclust(dist_correlation, method="complete")
plot(hc_c, cex=0.5, main="", sub="", xlab="")
## Applying hierarchical clustering using average linkage
hc_a = hclust(dist_correlation, method="average")
plot(hc_a, cex=0.5, main="", sub="", xlab="")
## 1C
## Applying hierarchical clustering using euclidean distance
dist_euclidean = dist(as.matrix(gexpr))
hc_se = hclust(dist_euclidean, method="single")
plot(hc_se, cex=0.5, main="", sub="", xlab="")
hc_se_cut = cutree(hc_se, h=45)
plot(pca_gexpr$x[,1], pca_gexpr$x[,1], xlab="First PC", ylab="Second PC", col=hc_se_cut, pch=hc_se_cut)
## 2A
Kmax = 10
SS_W = numeric(Kmax) # holds the values for within cluster and sum of squares
km_fit = list() # stores the km fit
for (K in 1:Kmax) {
# to find the minimum within-cluster sum-of-squares
km_fit[[K]] = kmeans(gexpr, K, iter.max = 50, nstart = 20)
SS_W[K] = km_fit[[K]]$tot.withinss # provides SSW for each value k
}
plot(1:Kmax, SS_W, type="l", xlab="K", ylab="SS_W")
## 2B
plot(pca_gexpr$x[,1], pca_gexpr$x[,1], xlab="First PC", ylab="Second PC", col=km_fit[[4]]$cluster, pch=km_fit[[4]]$cluster)
## Linear regressions
## A
train_set = diabetes[1:350,]
test_set = diabetes[350:nrow(diabetes),]
## B
## Response variable (measure of disease progression)
#y = train_set[,11]
#x = scale(as.matrix(train_set[,1:10]))
#diabetes_scale = data.frame(y, x)
#lsq_fit = lm(y~.,data=diabetes_scale)
#center = attr(x, "scaled:center")
#test_scale = scale(test_set, center, scale = attr(x, "scaled:scale"))
lsq_fit = lm(dis~., data = train_set)
yhat = predict(lsq_fit, test_set)
test_error = mean((test_set[,11] - yhat)^2)
## C
train_set_1 = train_set[c("sex","bmi","map","tc","ldl","ltg","dis")]
lsq_fit_1 = lm(dis~., data = train_set_1)
yhat_1 = predict(lsq_fit_1, test_set)
test_error_1 = mean((test_set[,11] - yhat_1)^2)
## D(i)
## i
lamdba = 10^seq(5,-3, length=100)
ridge_cv_fit = cv.glmnet(as.matrix(train_set[,1:10]), as.matrix(train_set[,11]),  alpha = 0, lamdba=lamdba)
ridge_lambda_min = ridge_cv_fit$lambda.min
## ii
## fit a regression model
ridge_fit = glmnet(as.matrix(train_set[,1:10]), as.matrix(train_set[,11]), alpha=0, lamdba=ridge_lambda_min)
yhat_ridge_fit = predict(ridge_fit, as.matrix(test_set[,1:10]), ridge_lambda_min)
test_error_ridge_fit = mean((test_set[,11] - yhat_ridge_fit)^2)
## D(ii)
## i
ridge_cv_fit_full = cv.glmnet(as.matrix(diabetes[,1:10]), as.matrix(diabetes[,11]), alpha = 0, lamdba=lamdba)
ridge_lambda_min_full = ridge_cv_fit_full$lambda.min
## ii
ridge_fit_full = glmnet(as.matrix(train_set[,1:10]), as.matrix(train_set[,11]), lamdba=lamdba)
plot(ridge_fit_full, xvar="lambda",label=TRUE)
### iii
## This the estimated regression coefficients for the ridge regression
beta_hat_ridge = coef(ridge_cv_fit_full)
beta_hat_ridge[,1]
## E
## -
lsq_fit = lm(dis~., data = train_set)
yhat = predict(lsq_fit, test_set)
test_error = mean((test_set[,11] - yhat)^2)
test_error_1
ridge_lambda_min
test_error_ridge_fit
ridge_lambda_min_full
plot(ridge_fit_full, xvar="lambda",label=TRUE)
ridge_lambda_min_full
plot(ridge_fit_full, xvar="lambda",label=TRUE)
plot(ridge_fit_full, xvar="lambda",label=TRUE)
beta_hat_ridge[,1]
beta_hat_ridge = coef(ridge_cv_fit_full)
beta_hat_ridge[,1]
ridge_fit_full = glmnet(as.matrix(train_set[,1:10]), as.matrix(train_set[,11]),alpha=0 lamdba=lamdba)
plot(ridge_fit_full, xvar="lambda",label=TRUE)
ridge_fit_full = glmnet(as.matrix(train_set[,1:10]), as.matrix(train_set[,11]),alpha=0 lamdba=lamdba)
plot(ridge_fit_full, xvar="lambda",label=TRUE)
ridge_fit_full = glmnet(as.matrix(train_set[,1:10]), as.matrix(train_set[,11]), alpha = 0, lamdba=lamdba)
plot(ridge_fit_full, xvar="lambda",label=TRUE)
ridge_fit_full = glmnet(as.matrix(train_set[,1:10]), as.matrix(train_set[,11]), lamdba=lamdba)
plot(ridge_fit_full, xvar="lambda",label=TRUE)
ridge_fit_full = glmnet(as.matrix(train_set[,1:10]), as.matrix(train_set[,11]), alpha = 0, lamdba=lamdba)
plot(ridge_fit_full, xvar="lambda",label=TRUE)
beta_hat_ridge = coef(ridge_cv_fit_full)
beta_hat_ridge[,1]
save.image("~/Documents/Msc Data science and AI/MAS8404 - Statistical Learning for Data Science /Assignment/env.RData")
## Load the require packages
library(mlbench)
library(bestglm)
library(glmnet)
library(nclSLR)
library(MASS)
set.seed(1)
## Load the data
data(BreastCancer)
## check size
dim(BreastCancer)
## -------------- Cleaning data ------------------
## 1. Changing class from 0 to 1
Cancer = data.frame(BreastCancer[,1:10], Class=as.integer(BreastCancer$Class)-1)
## 2. Remove all the rows which have is.na
Cancer = na.omit(Cancer)
## 3. Make all rows an integer
Cancer = data.frame(apply(Cancer, 2, as.numeric))
# Produce a plot to understand more about the data.
pairs(Cancer[2:11])
# Turn into data frame to be used for produce a logistic regression
# **Ensure ID (first column) is excluded, as it need needed for
# an explanatory variables**
X = scale(as.matrix(Cancer[,2:10]))
y = Cancer[,11]
cancer_data = data.frame(X,y)
## -------------- Apply a normal logistic regression ------------------
lr = glm(y ~ ., data=cancer_data, family=binomial)
summary(lr)
## -------------- Best subset selection ------------------
# Here we want to find the particular subset of parameters which would be best
# for providing the optimal results
n = nrow(cancer_data); p = ncol(cancer_data) -1
bss_fit_AIC = bestglm(cancer_data, family = binomial, IC = "AIC")
bss_fit_BIC = bestglm(cancer_data, family = binomial, IC = "BIC")
# we want to see the best predictors for a particular subset we examine the results
bss_fit_AIC$Subsets
bss_fit_BIC$Subsets
## This below is going to be used to find the best subset selection
# models which provide the best information criteria can be identify such:
best_AIC = bss_fit_AIC$ModelReport$Bestk
best_BIC = bss_fit_BIC$ModelReport$Bestk
# There are different criteria suggesting different models are best. Therefore we resort the plotting
# to get an idea about which model is the best
## For multiple linear regression, different criteria often suggest different models are best
## and this is the case here.
## In order to see the difference between the best model we can generate a plot
par(mfrow=c(1,2))
## produce plots, highlighting optimal value of k
plot(0:p, bss_fit_AIC$Subsets$AIC, xlab="Number of predictors", ylab="AIC", type="b")
points(best_AIC, bss_fit_AIC$Subsets$AIC[best_AIC+1], col="red", pch=16)
plot(0:p, bss_fit_BIC$Subsets$BIC, xlab = "Number of predictors", ylab = "BIC", type="b")
points(best_BIC, bss_fit_BIC$Subsets$BIC[best_BIC+1], col="red", pch=16)
## Get the best subset
bss_fit_AIC$Subsets[6,]
## create a linear regression with the optimal plots
cancer_data_bss = cancer_data[,c("Cl.thickness", "Marg.adhesion", "Bare.nuclei", "Bl.cromatin", "Normal.nucleoli", "y")]
lr_1 = glm(formula = y ~ ., family = "binomial", data = cancer_data_bss)
summary(lr_1)
## -------------- LASSO selection ------------------
par(mfrow=c(1,1))
## Here we want to using the LASSO to find the best optimal subset.
## Choose grid of values for the tuning parameter
grid = 10^seq(4,-4, length=100)
## Fit a model with LASSO penalty for each value of the tuning parameter
lasso_fit = glmnet(X, y, family="binomial", alpha=1, standardize=FALSE, lambda=grid)
plot(lasso_fit, xvar="lambda", col=rainbow(p), label=TRUE)
lasso_cv_fit = cv.glmnet(X, y, family="binomial", alpha=1, standardize=FALSE, lambda=grid, type.measure="class")
plot(lasso_cv_fit)
which_lambda_lasso = which(lasso_cv_fit$lambda == lambda_lasso_min)
lambda_lasso_min = lasso_cv_fit$lambda.min
coef(lasso_fit, s=lambda_lasso_min)
## -------------- ridge regression selection ------------------
## Fit a model with ridge regression penalty for each value of the tuning parameter
ridge_fit = glmnet(X, y, family="binomial", alpha=0, standardize=FALSE, lambda=grid)
plot(ridge_fit, xvar="lambda", col=rainbow(p), label=TRUE)
ridge_cv_fit = cv.glmnet(X, y, family="binomial", alpha=0, standardize=FALSE, lambda=grid, type.measure="class")
plot(ridge_cv_fit)
ridge_min = ridge_cv_fit$lambda.min
coef(ridge_fit, s=ridge_min)
## ------------- Calculates the test error -----------------
predictedProb = function(phat, y, yhat=NULL) {
#' Provides the testing error from a given
#' set of prediction, including a an output
#' of a concussion matrix.
#'
#'
#' @description This is done by taking the predictions
#'
#' @param phat The prediction
#' @param y The actual data
#' @param yhat The prediction
#'
#' @return The parameter theta
if (is.null(yhat) == TRUE) {
yhat = ifelse(phat > 0.5, 1, 0)}
print(table(Observed=y, Predicted=yhat))
return(1-mean(y==yhat))
}
## ------------- Linear discriminant analysis -----------------
lda_fit = linDA(variables = within(cancer_data, rm(y)), group = cancer_data$y)
lda_fit$functions
x = seq(-1, 0, 0.001)
x1 = seq(0, 1, 0.001)
Q1 = -1.8751-0.8757*x -0.6555*x -0.4576*x-0.2312*x -0.2193*x-1.6190*x-0.4599*x-0.5537*x-0.0166*x
Q2 = -6.0351 + 1.6269*x1+1.2177*x1+0.8500*x1+0.4294*x1+0.4073*x1+3.0077*x1+0.8543*x1+1.0287*x1+0.0308*x1
plot(x, Q2, type="l", col="red", xlab="x", ylab="Discriminant function")
lines(x, Q1, col="blue")
lda_train = lda(y ~ ., cancer_data)
## ------------- Quadratic discriminant analysis -----------------
qda_fit = quaDA(variables = within(cancer_data, rm(y)), group = cancer_data$y, functions = TRUE)
qda_train = qda(y ~ ., cancer_data)
qda_fit$functions
x_1 = seq(-5, 5, 0.001)
x_2 = seq(-5, 5, 0.001)
x_3 = seq(-5, 5, 0.001)
x_4 = seq(-5, 5, 0.001)
x_5 = seq(-5, 5, 0.001)
x_6 = seq(-5, 5, 0.001)
x_7 = seq(-5, 5, 0.001)
x_8 = seq(-5, 5, 0.001)
x_9 = seq(-5, 5, 0.001)
Q_1 = -0.719382*(x_1 )^2-1.7058039*(x_2 )^2-3.4937973*(x_3 )^2-0.58232003*(x_4 )^2-1.065855928*(x_5 )^2-0.8966413*(x_6 )^2-1.744926559*(x_7 )^2-0.84732991*(x_8 )^2-0.755990642*(x_9 )^2+ 0.5162133*(x_2 )+ 0.3404121*(x_3 )+ 0.2636177*(x_4 )+ 0.3156083*(x_5 )+ 1.5235343*(x_6 )+0.3832991*(x_7 )+ 0.5544928*(x_8 )-0.1372386*(x_9)-2.88806
Q_2 = -1.653368*(x_1 )^2-16.1466469*(x_2 )^2-24.779313*(x_3 )^2-7.67591883*(x_4 )^2-8.39889256*(x_5 )^2-16.5954618*(x_6)^2-4.52243149*(x_7)^2-20.43116132*(x_8 )^2-6.96453663*(x_9 )^2-0.1834093*(x_1 )  -2.8394269 *(x_2)-1.9059405*(x_3 )  -2.3386108*(x_4) -0.1733111*(x_5) -1.9083752*(x_6 )-1.4636685*(x_7) -1.1395618*(x_8 )  -3.0913668*(x_9 )+ 6.052617
plot(x_1, Q_1, type="l", col="red", xlab="x", ylab="Discriminant function")
lines(x_1, Q_2, col="blue")
## ------------- Cross validation -----------------
cross_valid = function(df = cancer_data, m = 1) {
#' Provides the testing error using cross validation
#'
#' @description There are 6 different models to select
#' from (1-6) specified using m. The model selected is
#' where cross validation will be applied:
#' The idea of the cross validation is as follows:
#' 1. Take the group as a holdout or test data set
#' 2. Take the remaining groups as a training data set
#' 3. Fit a model on the training set and evaluate it on the test set
#' 4. Retain the evaluation score and discard the model
#'
#' @param df The dataframe where cross validation is to be applied.
#' @param m The model selection include
#'        (1) Normal logistic regression
#'        (2) Logistic regression with best subset selection
#'        (3) Logistic regression with LASSO
#'        (4) Logistic regression with ridge regression
#'        (5) Discriminant based approach using LDA
#'        (6) Discriminant based approach using QDA
#'
#' @return list of evaluations for each n fold
evaluations = rep(0, 10)
# This is a list of splits to perform our training data and test data
split_num = list(c(1,69), c(69,137), c(137, 205), c(205,273), c(273,341), c(341,409), c(409,477), c(477,545),c(545,613),c(613, 683))
for ( i in seq(1:10) ) {
split = split_num[[i]]
test_data = df[split[1]:split[2],]
train_data = df[-(split[1]:split[2]),]
# The following if statement will check the mode
if (m == 1) {
model = glm(formula = y ~ ., family = "binomial", data = train_data)
evaluations[i] = predictedProb(predict(model, test_data, type="response"), test_data$y)
}
if (m == 2) {
model = glm(formula = y ~ ., family = "binomial", data = train_data[,c("Cl.thickness", "Marg.adhesion", "Bare.nuclei", "Bl.cromatin", "Normal.nucleoli", "y")] )
evaluations[i] = predictedProb(predict(model, test_data[,c("Cl.thickness", "Marg.adhesion", "Bare.nuclei", "Bl.cromatin", "Normal.nucleoli", "y")], type="response"), test_data$y)
}
if (m == 3) {
grid = 10^seq(4,-4, length=100)
data = within(train_data, rm(y))
model = glmnet(as.matrix(data), as.matrix(train_data$y), family="binomial", alpha=1, standardize=FALSE, lambda=grid)
fit = cv.glmnet(as.matrix(data), as.matrix(train_data$y), family="binomial", alpha=1, standardize=FALSE, lambda=grid, type.measure="class")
lambda_min = fit$lambda.min
evaluations[i] = predictedProb(predict(model, as.matrix(within(test_data, rm(y))), s=lambda_min, type="response"), test_data$y)
}
if (m == 4) {
grid = 10^seq(4,-4, length=100)
data = within(train_data, rm(y))
model = glmnet(as.matrix(data), as.matrix(train_data$y), alpha=0, family="binomial", standardize=FALSE, lambda=grid)
fit = cv.glmnet(as.matrix(data), as.matrix(train_data$y), alpha=0, family="binomial", standardize=FALSE, lambda=grid, type.measure="class")
lambda_min = fit$lambda.min
evaluations[i] = predictedProb(predict(model, as.matrix(within(test_data, rm(y))), s=lambda_min, type="response"), test_data$y)
}
if (m == 5) {
model = lda(y ~ ., train_data)
testing = predict(model, test_data, type="response")
evaluations[i] = predictedProb(testing, test_data$y, testing$class )
}
if (m == 6) {
model = qda(y ~ ., train_data)
testing = predict(model, test_data, type="response")
evaluations[i] = predictedProb(testing, test_data$y, testing$class )
}
}
return(evaluations)
}
## ---------------- Predicted using full set ---------------------
predictedProb(predict(lasso_fit, X, s=lambda_lasso_min, type="response"), y)
predictedProb(predict(ridge_fit, X, s=ridge_min, type="response"), y)
predictedProb(predict(lr_1, cancer_data_bss, type="response"), y)
lda_testing = predict(lda_train, cancer_data, type="response")
predictedProb(lda_testing, y, lda_testing$class )
qda_testing = predict(qda_train, cancer_data, type="response")
predictedProb(qda_testing, y, qda_testing$class )
## --------------- Predicted using cv ----------------------
num = cross_valid(m=2)
num
sum(num)/10
nrow(cancer_data[-(613:683),])
length(cancer_data$y[cancer_data$y == 1])
setwd("~/Desktop")
setwd("~/Desktop/learning_analysis")
clear.cache()
clear.cache()
load.project()
setwd("~/Desktop/learning_analysis")
clear.cache()
load.project()
\
library("ProjectTemplate")
clear.cache()
load.project()
View(code)
